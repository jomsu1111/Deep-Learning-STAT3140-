# ğŸ“¦ ì œí’ˆ ë¦¬ë·° ê¸°ë°˜ í‰ì  ì˜ˆì¸¡ ëª¨ë¸ (Rating Prediction Project)

[cite_start]ì´ í”„ë¡œì íŠ¸ëŠ” Amazon ì œí’ˆì˜ ì¶œì‹œ ì´ˆê¸° ë¦¬ë·° í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬, í•´ë‹¹ ì œí’ˆì˜ ì¥ê¸°ì ì¸ **í‰ê·  í‰ì ** ì„ ì˜ˆì¸¡í•˜ëŠ” ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ êµ¬ì¶•í•œ ì—°êµ¬ì…ë‹ˆë‹¤[cite: 9, 116]. [cite_start]ì‚¬ìš©ì ì •ë³´ë‚˜ ë©”íƒ€ë°ì´í„° ì—†ì´ ìˆœìˆ˜ ìì—°ì–´ ì²˜ë¦¬ ê¸°ë²•ë§Œìœ¼ë¡œ ì‹œì¥ ë°˜ì‘ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤[cite: 11, 117].

## 1. í”„ë¡œì íŠ¸ ê°œìš” (Overview)
* [cite_start]**ë¬¸ì œ ì •ì˜**: ì‹ ìƒí’ˆ ì¶œì‹œ ì´ˆê¸°, ë¶€ì¡±í•œ ë¦¬ë·° ìˆ˜ë¡œ ì¸í•œ ì†Œë¹„ìì˜ ì‹ ë¢°ë„ ì €í•˜ ë° ì‹œì¥ ë°˜ì‘ íŒŒì•…ì˜ ì–´ë ¤ì›€[cite: 7, 8].
* [cite_start]**í•µì‹¬ ê°€ì„¤**: ì´ˆê¸° 5~10ê°œì˜ ë¦¬ë·°ì— ë‹´ê¸´ ê°ì„±ì /ì–¸ì–´ì  íŠ¹ì§•ì´ ì œí’ˆì˜ ìµœì¢… í‰ì ê³¼ ë†’ì€ ìƒê´€ê´€ê³„ë¥¼ ê°€ì§ˆ ê²ƒì´ë‹¤[cite: 10, 14].
* [cite_start]**ê¸°ëŒ€ íš¨ê³¼**: ì´ì»¤ë¨¸ìŠ¤ í”Œë«í¼ì—ì„œ ì‹ ìƒí’ˆ ì¶œì‹œ ì „ëµ ìˆ˜ë¦½ ë° ì´ˆê¸° ëŒ€ì‘ì„ ìœ„í•œ ì‹¤ì§ˆì  ë°ì´í„° ì œê³µ[cite: 12, 117].

## 2. ë°ì´í„° ë° ì „ì²˜ë¦¬ (Data & Preprocessing)
* [cite_start]**ë°ì´í„°ì…‹**: 2023ë…„ Amazon ì œí’ˆ ë¦¬ë·° ë°ì´í„° (**asin** , **text** , **rating** , **time_stamp** í¬í•¨)[cite: 20, 21].
* **ë°ì´í„° ì„ ë³„**: 
    * [cite_start]ìµœì†Œ 10ê±´ ì´ìƒì˜ ë¦¬ë·°ë¥¼ ë³´ìœ í•œ ìƒí’ˆ ì„ ë³„[cite: 44, 45].
    * [cite_start]ì œí’ˆ ì¶œì‹œ í›„ 6ê°œì›” ì´ë‚´ì˜ ë¦¬ë·°ë§Œì„ 'ì´ˆê¸° ë¦¬ë·°'ë¡œ ê°„ì£¼í•˜ì—¬ ì¶”ì¶œ[cite: 46, 47].
* **í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬**:
    * [cite_start]ì†Œë¬¸ì ë³€í™˜, URL, íŠ¹ìˆ˜ë¬¸ì, ë¶ˆìš©ì–´(**stopwords** ) ì œê±°[cite: 49].
    * [cite_start]**Keras Tokenizer** ë¥¼ í†µí•´ ìƒìœ„ 10,000ê°œ ë‹¨ì–´ ì‚¬ìš© ë° ê¸¸ì´ 500ìœ¼ë¡œ íŒ¨ë”© ì²˜ë¦¬[cite: 50, 51].
* [cite_start]**ë°ì´í„° êµ¬ì„±**: ê°œë³„ ì‚¬ìš©ìì˜ ë¦¬ë·°ë¥¼ í•˜ë‚˜ë¡œ ê²°í•©(**concatenate** )í•˜ì—¬ ìƒí’ˆ ì „ì²´ì— ëŒ€í•œ ì¢…í•© í‰ê°€ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜[cite: 106, 107].

## 3. ëª¨ë¸ ì•„í‚¤í…ì²˜ (Model Architecture)
[cite_start]ëª¨ë¸ì€ **LSTM** ê³¼ **Attention** ë©”ì»¤ë‹ˆì¦˜ì„ ê²°í•©í•œ êµ¬ì¡°ë¡œ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤[cite: 56, 118].

* [cite_start]**Embedding Layer**: 10,000ê°œ ë‹¨ì–´ë¥¼ 64ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜í•˜ë©°, ë¦¬ë·° ë°ì´í„°ì— ì í•©í•œ í‘œí˜„ì„ ìŠ¤ìŠ¤ë¡œ í•™ìŠµí•©ë‹ˆë‹¤[cite: 61, 121].
* **LSTM Layers**: 
    * [cite_start]ì²« ë²ˆì§¸ ì¸µ: 64 units, ì „ì²´ ì‹œí€€ìŠ¤ ë°˜í™˜ (**return_sequences=True** )[cite: 64].
    * [cite_start]ë‘ ë²ˆì§¸ ì¸µ: 32 units, ì‹œí€€ìŠ¤ ì •ë³´ ì••ì¶•[cite: 65].
    * [cite_start]ê° ê³„ì¸µ ì‚¬ì´ **Dropout** ì„ ì ìš©í•˜ì—¬ ê³¼ì í•©(**Overfitting** ) ë°©ì§€[cite: 66, 119].
* [cite_start]**Attention Mechanism**: **Self-Attention** êµ¬ì¡°ë¥¼ í†µí•´ ì‹œí€€ìŠ¤ ë‚´ ì¤‘ìš”í•œ ë‹¨ì–´ì— ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ê³  í•µì‹¬ ë¬¸ë§¥ ë²¡í„°(**Context Vector** )ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤[cite: 70, 72].

## 4. í•™ìŠµ ê²°ê³¼ ë° ì„±ëŠ¥ (Results)
* **ì„±ëŠ¥ ì§€í‘œ**:
    * [cite_start]**MAE (Mean Absolute Error)** : **0.3893** (ëª©í‘œì¹˜ 0.5 ì´ë‚´ ë‹¬ì„±)[cite: 81, 122].
    * [cite_start]**MAPE (Mean Absolute Percentage Error)** : **11.19%**[cite: 84, 122].
* **ê²°ê³¼ í•´ì„**:
    * [cite_start]3.5ì  ì´ìƒì˜ ë†’ì€ í‰ì ëŒ€ì—ì„œ ì‹¤ì œê°’ê³¼ ì˜ˆì¸¡ê°’ì˜ ì¼ì¹˜ë„ê°€ ë§¤ìš° ë†’ìŒ[cite: 91].
    * [cite_start]ì”ì°¨ ë¶„ì„ ê²°ê³¼, ì˜¤ì°¨ê°€ 0ì„ ì¤‘ì‹¬ìœ¼ë¡œ ëŒ€ì¹­ì ì¸ ì •ê·œë¶„í¬ë¥¼ í˜•ì„±í•˜ì—¬ ëª¨ë¸ì˜ ì•ˆì •ì„± í™•ì¸[cite: 93, 124].

## 5. ê²°ë¡  ë° ì œì–¸ (Conclusion)
* [cite_start]**ì„±ëŠ¥ ê²€ì¦**: ê°„ë‹¨í•œ êµ¬ì¡°ì˜ **LSTM** ë° **Attention** ê²°í•© ëª¨ë¸ë§Œìœ¼ë¡œë„ ìœ ì˜ë¯¸í•œ ì˜ˆì¸¡ ì •í™•ë„ í™•ë³´ê°€ ê°€ëŠ¥í•¨ì„ ì…ì¦í•¨[cite: 127].
* **í–¥í›„ ê°œì„  ë°©í–¥**:
    * [cite_start]**Transformer** ê¸°ë°˜ ëª¨ë¸ ë„ì…ì„ í†µí•œ ì¥ê¸° ì‹œí€€ìŠ¤ ì²˜ë¦¬ ëŠ¥ë ¥ ê³ ë„í™”[cite: 129].
    * [cite_start]í‰ì  ë¶ˆê· í˜• í•´ì†Œë¥¼ ìœ„í•œ ìƒ˜í”Œë§ ê¸°ë²• ë° ì†ì‹¤ í•¨ìˆ˜(**Loss Function** ) ì¡°ì •[cite: 131].

## 6. ì°¸ì¡° (Reference)
* [cite_start][Amazon Reviews Data 2023 (McAuley Lab)](https://www.kaggle.com/datasets/wajahat1064/amazon-reviews-data-2023) [cite: 135]
* [cite_start][Attention Mechanism (Wikidocs)](https://wikidocs.net/22893) [cite: 136]
* [cite_start][Keras Tokenizer (Wikidocs)](https://wikidocs.net/182469) [cite: 137]
